---
title: "Lab 12"
author: "Skyler Moomey"
date: '`r format(Sys.Date(),format="%A, %B %d, %Y")`'
output: 
  html_document:
    df_print: paged
    fig_caption: true
    highlights: pygments
    number_sections: no
    theme: spacelab
    toc: yes
    toc_float: yes
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# Tasks

## Task 1


### getwd()
```{r}
getwd()
```

## Task 2

### One-Sample Hypothesis Testing

```{r}
set.seed(55);x1=rnorm(30,mean=25,sd=5)

t.test(x1, mu=22)
t.test(x1, mu=23)
t.test(x1, mu=24)
t.test(x1, mu=25)
t.test(x1, mu=26)
```
For $\mu_0=22$, the null is rejected, $p=0.002$.
For $\mu_0=23$, the null is rejected, $p=0.025$.
For $\mu_0=24$, the null is not rejected, $p=0.1951$.
For $\mu_0=25$, the null is not rejected, $p=0.7692$.
For $\mu_0=26$, the null is not rejected, $p=0.469$.

### Box-plots

```{r}
boxplot(x1, ylab="Mean", ylim=c(0, max(x1)+2))
title(main="Sample x1", col.main="purple")
ci=t.test(x1, conf.level=.95)$conf.int
abline(h=c(ci[1], ci[2]), col="red")
abline(h=mean(x1), col="green")
```

### P-Values

```{r}
# Display P-value areas
mypvalue=function(t0,xmax=4,n=20, alpha=0.05){
#calculate alpha/2
va=round(pt(-t0,df=n-1),4)
pv=2*va

# plot the t dist
curve(dt(x,df=n-1),xlim=c(-xmax,xmax),ylab="T Density",xlab=expression(t),
main=substitute(paste("P-value=", pv, " alpha=", alpha)))


# set up points on the polygon to the right
xcurve=seq(t0,xmax,length=1000)
ycurve=dt(xcurve,df=n-1)

# set up points to the left
xlcurve=seq(-t0,-xmax,length=1000)
ylcurve=dt(xcurve,df=n-1)

# Shade in the polygon defined by the line segments
polygon(c(t0,xcurve,xmax),c(0,ycurve,0),col="green")
polygon(c(-t0,xlcurve,-xmax),c(0,ylcurve,0),col="green")

# make quantiles
q=qt(1-alpha/2,n-1)
abline( v=c(q,-q),lwd=2) # plot the cut off t value 
axis(3,c(q,-q),c(expression(abs(t[alpha/2])),expression(-abs(t[alpha/2]))))


# Annotation
text(0.5*(t0+xmax),max(ycurve),substitute(paste(area, "=",va)))
text(-0.5*(t0+xmax),max(ycurve),expression(area))

return(list(q=q,pvalue=pv))
}
```

```{r}
tcalc=(mean(x1)-24)/(sd(x1)/sqrt(length(x1)))
mypvalue(tcalc)
```
The rejection region is determined by $t \leq t_{0.025}$ and $t \geq t_{1-0.025}$. The $p$-value necessary to reject $H_0$ is 0.05. Our p-value, $p=0.2004$ is larger than necessary, and our $t_{calc}$ is not within the rejection area. Therefore, we do not reject the null hypothesis.

### Bootstrap P-Values

```{r}
bootpval<-function(x,conf.level=0.95,iter=3000,mu0=0, test="two"){
n=length(x)
y=x-mean(x)+mu0  # transform the data so that it is centered at the NULL
rs.mat<-c()    #rs.mat will become a resample matrix -- now it is an empty vector
xrs.mat<-c()
for(i in 1:iter){ # for loop - the loop will go around iter times
rs.mat<-cbind(rs.mat,sample(y,n,replace=TRUE)) #sampling from y cbind -- column bind -- binds the vectors together by columns
xrs.mat<-cbind(xrs.mat,sample(x,n,replace=TRUE)) #sampling from x cbind -- column bind -- binds the vectors together by columns

}

tstat<-function(z){ # The value of t when the NULL is assumed true (xbar-muo)/z/sqrt(n)
sqrt(n)*(mean(z)-mu0)/sd(z)
}

tcalc=tstat(x) # t for the data collected
ytstat=apply(rs.mat,2,tstat) # tstat of resampled y's, ytstat is a vector and will have iter values in it
xstat=apply(xrs.mat,2,mean)  # mean of resampled x's
alpha=1-conf.level # calculating alpha
ci=quantile(xstat,c(alpha/2,1-alpha/2))# Nice way to form a confidence interval
pvalue=ifelse(test=="two",length(ytstat[ytstat>abs(tcalc) | ytstat < -abs(tcalc)])/iter,
ifelse(test=="upper",length(ytstat[ytstat>tcalc])/iter,
length(ytstat[ytstat<xstat])/iter))

h=hist(ytstat,plot=FALSE)
mid=h$mid
if(test=="two"){
ncoll=length(mid[mid<= -abs(tcalc)])
ncolr=length(mid[mid>=  abs(tcalc)])
col=c(rep("Green",ncoll),rep("Gray",length(mid)-ncoll-ncolr),rep("Green",ncolr))
}
if(test=="upper"){
ncolr=length(mid[mid>=  abs(tcalc)])
col=c(rep("Gray",length(mid)-ncolr),rep("Green",ncolr))
}

if(test=="lower"){
ncoll=length(mid[mid<=  -abs(tcalc)])
col=c(rep("Green",ncoll),rep("Gray",length(mid)-ncoll))
}
hist(ytstat,col=col,freq=FALSE,las=1,main="",xlab=expression(T[stat]))
#segments(ci[1],0,ci[2],0,lwd=2)
pround=round(pvalue,4)
title(substitute(paste(P[value],"=",pround)))
return(list(pvalue=pvalue,tcalc=tcalc,n=n,x=x,test=test,ci=ci))
}
```

```{r}
bootpval(x1, mu0=22)
bootpval(x1, mu0=23)
bootpval(x1, mu0=24)
bootpval(x1, mu0=25)
bootpval(x1, mu0=26)
```

Just as before, the function leads us to reject $\mu_0=22,23$, but we find no reason to reject the other null hypotheses, $\mu_0=24,25,26$.

## Task 3

### Equality of Variance, sd1=7, sd2=4

```{r}
set.seed(30);x=rnorm(15,mean=10,sd=7)   
set.seed(40);y=rnorm(20,mean=12,sd=4)

var.test(x, y)
```

The $p$-value for the variance test was $p=0.016 \leq \alpha=0.05$. Therefore, we reject the null hypothesis. For a T-Test, we would say that the variances are not equal.

### t.test

```{r}
t.test(x,y, var.equal=FALSE, mu=0)
t.test(x,y, var.equal=FALSE, mu=-2)
```

Based on these two t-tests, the assumption that $\mu_x = \mu_y$ is not plausibly true, so we reject that. For the assumption $\mu_y-\mu_x=2$, our $p$-value was sufficiently high. Therefore, we accept the null hypothesis that they differ by 2.

## Task 4

### Equality of Variance, sd1=sd2=4

```{r}
set.seed(30);x=rnorm(15,mean=10,sd=4)   
set.seed(40);y=rnorm(20,mean=12,sd=4)
var.test(x, y)
```

Our $p$-value is sufficiently high to plausibly assume that our population variances equal.

### t.test

```{r}
t.test(x, y, var.equal=TRUE, mu=0)
t.test(x, y, var.equal=TRUE, mu=-2)
```

For $\alpha=0.05$, our p-values were sufficiently low as to plausibly reject the first null hypothesis and retain the second. Thus, we accept the possibility that $\mu_y-\mu_x=2$.

## Task 5

### boot2pval()

```{r}
boot2pval<-function(x1,x2,conf.level=0.95,iter=3000,mudiff=0, test="two"){
n1=length(x1)
n2=length(x2)
y1=x1-mean(x1)+mean(c(x1,x2))  # transform the data so that it is centered at the NULL
y2=x2-mean(x2)+mean(c(x1,x2))
y1rs.mat<-c()    #rs.mat will be come a resample matrix -- now it is an empty vector
x1rs.mat<-c()
y2rs.mat<-c()
x2rs.mat<-c()
for(i in 1:iter){ # for loop - the loop will go around iter times
y1rs.mat<-cbind(y1rs.mat,sample(y1,n1,replace=TRUE)) #sampling from y cbind -- column bind -- binds the vectors together by columns
y2rs.mat<-cbind(y2rs.mat,sample(y2,n2,replace=TRUE))

}
x1rs.mat<-y1rs.mat+mean(x1)-mean(c(x1,x2))
x2rs.mat<-y2rs.mat+mean(x2)-mean(c(x1,x2))

xbar1=mean(x1)
xbar2=mean(x2)
sx1sq=var(x1)
sx2sq=var(x2)

tcalc=(xbar1-xbar2-mudiff)/sqrt(sx1sq/n1+sx2sq/n2)

sy1sq=apply(y1rs.mat,2,var)
sy2sq=apply(y2rs.mat,2,var) 
y1bar=apply(y1rs.mat,2,mean)
y2bar=apply(y2rs.mat,2,mean)

tstat=(y1bar-y2bar-mudiff)/sqrt(sy1sq/n1+sy2sq/n2)


alpha=1-conf.level # calculating alpha
#ci=quantile(xstat,c(alpha/2,1-alpha/2))# Nice way to form a confidence interval
pvalue=ifelse(test=="two",length(tstat[tstat>abs(tcalc) | tstat < -abs(tcalc)])/iter,
ifelse(test=="upper",length(tstat[tstat>tcalc])/iter,
length(ytstat[tstat<tcalc])/iter))

h=hist(tstat,plot=FALSE)
mid=h$mid
if(test=="two"){
ncoll=length(mid[mid<= -abs(tcalc)])
ncolr=length(mid[mid>=  abs(tcalc)])
col=c(rep("Green",ncoll),rep("Gray",length(mid)-ncoll-ncolr),rep("Green",ncolr))
}
hist(tstat,col=col,freq=FALSE)
#segments(ci[1],0,ci[2],0,lwd=2)

return(list(pvalue=pvalue))
#return(list(pvalue=pvalue,tcalc=tcalc,n=n,x=x,test=test,ci=ci))
}
```

### Samples from Task 3, Bootstrap

```{r}
set.seed(30);x=rnorm(15,mean=10,sd=7)   
set.seed(40);y=rnorm(20,mean=12,sd=4)

boot2pval(x1=y, x2=x)
boot2pval(x1=y, x2=x, mudiff=2)
```
$p_1$ for $H_0: \mu_y - \mu_x = 0$ is $0.084$

$p_2$ for $H_0: \mu_y - \mu_x = 2$ is $0.587$

Therefore, we find no evidence against either Null Hypothesis, though hypothesis 2 may be more plausible.

## Task 6

### Bootstrap, Samples from Task 4

```{r}
set.seed(30);x=rnorm(15,mean=10,sd=4)   
set.seed(40);y=rnorm(20,mean=12,sd=4)

boot2pval(y, x, mudiff=0)
boot2pval(y, x, mudiff=2)
```
In this case, the $p$-value for $H_0: \mu_y - \mu_x = 0$ was sufficiently though as to be rejected. We retain the second null hypothesis.

## Task 7

Line A calls the t.test function on a single sample with the null hypothesis that $\mu=23$

Line B is just the function explaining that it is a One-Sample test

Line C gives the following sample statistics, $t_{stat}=2.3653, \quad df=29,$ and $p-value=0.02543$. This means that our standarized mean was about 2.3653 standard deviations away from $\mu=23$, which is quite far. Our p-value suggests that we should reject the null hypothesis.

Line D is a statement of the alternative hypothesis, that is, $\mu \neq 23$.

Line F shows the bounds on our $95\%$ confidence interval for the population mean. This is formulated using the equation $\bar y \pm t_{0.025}\frac{s}{\sqrt{n}}$.

Line G shows that our sample mean, $s=25.28759$.

## Task 8

```{r}
library(MATH4753moom0002)

x = rnorm(56, mean=12, sd=2)
MATH4753moom0002::bootpval(x, conf.level=.95, iter=5000, mu0=12)
MATH4753moom0002::bootpval(x, conf.level=.95, iter=5000, mu0=11)
```


